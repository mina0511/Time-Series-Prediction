{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from seaborn import heatmap\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tools import validation\n",
    "from models import Model1, Model2, Model3, Model4, Model5, Model6\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "device = torch.device('cuda:1')\n",
    "criterion = nn.MSELoss()\n",
    "eval_metrics = [r2_score, mean_absolute_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_dataset_masked.pickle', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LSTNet import LSTNet\n",
    "lstnet_best = LSTNet(P=60, m=40, dropout=0.2, output_func='sigmoid', device=device, hidR=32, hidC=32, hidS=40, Ck=10, hw=1).to(device)\n",
    "lstnet_state_dict = torch.load('saved_model_masked/LSTNet_best.pt', map_location='cuda:1')\n",
    "lstnet_best.load_state_dict(lstnet_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 \t 160 40\n",
      "lstm.weight_hh_l0 \t 160 40\n",
      "lstm.bias_ih_l0 \t 160\n",
      "lstm.bias_hh_l0 \t 160\n",
      "lstm.weight_ih_l1 \t 160 40\n",
      "lstm.weight_hh_l1 \t 160 40\n",
      "lstm.bias_ih_l1 \t 160\n",
      "lstm.bias_hh_l1 \t 160\n",
      "lstm.weight_ih_l2 \t 160 40\n",
      "lstm.weight_hh_l2 \t 160 40\n",
      "lstm.bias_ih_l2 \t 160\n",
      "lstm.bias_hh_l2 \t 160\n",
      "linear_output.0.weight \t 80 40\n",
      "linear_output.0.bias \t 80\n",
      "linear_output.2.weight \t 1 80\n",
      "linear_output.2.bias \t 1\n"
     ]
    }
   ],
   "source": [
    "slstm_state_dict = torch.load('checkpoints/Simple_LSTM_best.pt')\n",
    "for name, data in slstm_state_dict.items():\n",
    "    print(name, '\\t', *data.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Attention + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_lstm_state_dict = torch.load('checkpoints/Input_Attention_LSTM_masked_best.pt')\n",
    "for name, data in ia_lstm_state_dict.items():\n",
    "    print(name, '\\t', *data.size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_best = Model1(T=60, n=40, m=40, cnn_kernel_height=10, cnn_hidden_size=32, skip_hidden_size=40, skip=8).to(device)\n",
    "model1_state_dict = torch.load('saved_model_masked/model1_best.pt')\n",
    "model1_best.load_state_dict(model1_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_best = Model2(T=60, n=40, m=32, skip_hidden_size=40, T_modified=20, skip=10).to(device)\n",
    "model2_state_dict = torch.load('saved_model_masked/model2_best.pt')\n",
    "model2_best.load_state_dict(model2_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_best = Model3(T=60, n=40, m=40, skip_hidden_size=40, T_modified=20, skip=8).to(device)\n",
    "model3_state_dict = torch.load('saved_model_masked/model3_best.pt')\n",
    "model3_best.load_state_dict(model3_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_best = Model4(T=60, n=40, m=40, skip_hidden_size=40, skip=10).to(device)\n",
    "model4_state_dict = torch.load('saved_model_masked/model4_best.pt', map_location='cuda:1')\n",
    "model4_best.load_state_dict(model4_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5_best = Model5(T=60, n=40, p=32, cnn_kernel_height=30, cnn_hidden_size=32, skip_hidden_size=40, skip=8).to(device)\n",
    "model5_state_dict = torch.load('saved_model_masked/model5_best.pt', map_location='cuda:1')\n",
    "model5_best.load_state_dict(model5_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_best = Model6(T=60, n=40, m=40, T_modified=20).to(device)\n",
    "model6_state_dict = torch.load('saved_model_masked/model6_best.pt', map_location='cuda:1')\n",
    "model6_best.load_state_dict(model6_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_grid_search_result = []\n",
    "\n",
    "lstnet_best.eval()\n",
    "model6_best.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    unmasked_grid_search_result.append(validation(lstnet_best, test_loader, criterion, eval_metrics, device))\n",
    "    unmasked_grid_search_result.append(validation(model1_best, test_loader, criterion, eval_metrics, device))\n",
    "    unmasked_grid_search_result.append(validation(model2_best, test_loader, criterion, eval_metrics, device))\n",
    "    unmasked_grid_search_result.append(validation(model3_best, test_loader, criterion, eval_metrics, device))\n",
    "    unmasked_grid_search_result.append(validation(model4_best, test_loader, criterion, eval_metrics, device))\n",
    "    unmasked_grid_search_result.append(validation(model5_best, test_loader, criterion, eval_metrics, device))\n",
    "    unmasked_grid_search_result.append(validation(model6_best, test_loader, criterion, eval_metrics, device))\n",
    "\n",
    "unmasked_grid_search_result = pd.DataFrame(unmasked_grid_search_result, columns=['Loss', 'R2', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked_grid_search_result['Model'] = ['LSTNet'] + [f'Model {i}' for i in range(1, 7)]\n",
    "unmasked_grid_search_result = unmasked_grid_search_result.iloc[:, [3, 0, 1, 2]]\n",
    "unmasked_grid_search_result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Score Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in test_loader:\n",
    "    X = X.float().to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_scores(model, X, X_index, kind='Input', masked=False, save_filename=None):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            model(X)\n",
    "            attention_scores_ = model.attention_scores_.cpu().detach() # (-1 x T x `dim`)\n",
    "            # `dim` means the dimension of features of input to (Input or Temporal) Attention\n",
    "        attention_scores = attention_scores_[X_index] # (T x `dim`)\n",
    "\n",
    "        fig1, sub1 = plt.subplots(1, 1, dpi=100, figsize=(7, 5))\n",
    "        heatmap(attention_scores, cmap='Reds', vmin=0, vmax=1, ax=sub1)\n",
    "        sub1.set_ylabel('Time steps')\n",
    "        sub1.set_xlabel(f'{kind} Features')\n",
    "\n",
    "        fig2 = None\n",
    "        if masked:\n",
    "            fig2, sub = plt.subplots(1, 1, dpi=100, figsize=(7, 10))\n",
    "            sub.hist(attention_scores[X[X_index] == 0], label='masked', bins=15, histtype='step')\n",
    "            sub.hist(attention_scores[X[X_index] != 0], label='unmasked', bins=15, histtype='step')\n",
    "        \n",
    "        if save_filename:\n",
    "            fig1.savefig(f'{save_filename}')\n",
    "            print(f'Saving Process Complete. Directory: heatmap_{save_filename}')\n",
    "            if fig2 is not None:\n",
    "                fig2.savefig(f'{save_filename}')\n",
    "                print(f'Saving Process Complete. Directory: hist_{save_filename}')\n",
    "        \n",
    "        return attention_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_attention_scores(model1_best, X, 0, kind='Input', masked=False, save_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_attention_scores(model2_best, X, 0, kind='Input', masked=False, save_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_attention_scores(model3_best, X, 0, kind='Input', masked=False, save_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_attention_scores(model4_best, X, 0, kind='Input', masked=False, save_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_attention_scores(model5_best, X, 0, kind='Input', masked=False, save_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_attention_scores(model6_best, X, 0, kind='Input', masked=False, save_filename=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kwon_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7aba8400f5793028c76b760b8f218e34a1395ac8226b95289931317412384abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
